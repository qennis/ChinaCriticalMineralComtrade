# ChinaCriticalMineralComtrade

Pipeline for analyzing China’s exports of critical-mineral–related products using UN Comtrade data, with a focus on structure (HS6-level concentration), export controls, and price–volume dynamics.

This repository accompanies my empirical project on China’s export controls on critical minerals. You can find the most recent draft of the paper in `paper/`.

The repo does three main things:

1. **Pulls** monthly HS6 trade data for a curated set of “critical” codes from the Comtrade v1 API.
2. **Builds** cleaned, grouped panel datasets at both HS6 and “material group” level.
3. **Generates** a standard set of figures for trade levels, composition, concentration, export-control event windows, and price–volume regimes.

---

## Repository structure

The important bits:

* `src/china_ir/`

  * `comtrade.py` – thin client for the Comtrade v1 API and helpers to fetch mapped HS6 data.
  * `etl.py` – ETL pipeline: attach HS6→group mapping, aggregate to groups, compute HHIs, unit values, and analysis panels.
* `scripts/`

  * `pull_comtrade.py` – low-level Comtrade pull (rarely used directly now).
  * `pull_from_map.py` – **main entry point** to fetch all mapped HS6 data for a given period.
  * `make_figures.py` – builds all the figures from processed parquet tables.
* `notes/`

  * `hs_map.csv` – HS6→material group mapping (aluminum, graphite, lithium, etc.).
* `data_work/`

  * Raw pulled parquet files and derived tables (created by the pipeline).
* `figures/`

  * PNGs generated by `make_figures.py`.

---

## Installation

You’ll need Python 3.10+ (3.11 works) and standard scientific Python packages.

From the repo root:

```bash
# (optional) create a dedicated environment
conda create -n china-ir python=3.11
conda activate china-ir

# install dependencies
conda env create -f environment.yml
conda activate china-ir
```

The code assumes it’s run from the repository root (where `src/` lives).

---

## Quickstart: end-to-end workflow

All commands below are run from the repo root.

### 1. Pull mapped Comtrade data

`pull_from_map.py` fetches Comtrade HS6 data for all HS codes listed in `notes/hs_map.csv` and writes parquet files into `data_work/`.

Basic usage:

```bash
PYTHONPATH=src ./scripts/pull_from_map.py \
  --freq M \
  --period 201801-202412 \
  --flow X \
  --reporter 156 \
  --partner 0
```

Key arguments:

* `--freq {A,M}` – annual or monthly; most analysis uses `M`.
* `--period` – either a single `YYYY`/`YYYYMM` or a hyphenated range like `201801-202412`.
* `--flow {X,M}` – exports or imports (analysis uses `X`).
* `--reporter` – reporter code; `156` is China.
* `--partner` – partner code:

  * `0` for world (current pipeline uses this),
  * `ALL` for all partners,
  * or a numeric partner (e.g. `842` for US).
* `--map` (optional) – override path to `hs_map.csv`.
* `--chunk` (optional) – split long period ranges into chunks for the API.

Outputs look like:

```text
data_work/comtrade_CM_HS_201801-202412_156_0_X_MAP_<timestamp>.parquet
```

You can repeat this step with different `--partner` values if you later extend the ETL to use destination-level data.

---

### 2. Build analysis tables

`etl.py` reads the raw parquet pulls, attaches the HS6→group mapping, and writes cleaned tables.

Example:

```bash
python src/china_ir/etl.py \
  --in-glob 'comtrade_CM_HS_*_MAP_*.parquet' \
  --hs-map notes/hs_map.csv \
  --outdir data_work
```

Arguments:

* `--in-glob` – glob pattern (relative to `data_work/`) for raw parquet files.
* `--hs-map` – path to `hs_map.csv`.
* `--outdir` – where to write processed parquet tables.

Typical outputs:

* `data_work/materials_monthly.parquet`
  Monthly panel by group:

  * `period` (YYYYMM), `group`, `material`,
  * `value`, `qty`, `total` (basket total), `share` (`value/total`).

* `data_work/materials_annual.parquet`
  Annual aggregates by group.

* `data_work/materials_hhi.parquet`
  Group-level HHI and effective number of groups by month.

* `data_work/hs6_hhi.parquet`
  HS6-level HHI within each group (effective number of HS6 lines by group × month).

(If destination-level logic is wired in, you may also see `dest_hhi.parquet` and similar, but the current pipeline primarily uses the four above.)

---

### 3. Generate figures

`make_figures.py` reads the processed parquet tables and builds a standard figure set into `figures/`.

From the repo root:

```bash
python scripts/make_figures.py
```

This script takes care of adding `src/` to `sys.path` so that `from china_ir.etl import ...` works.

After running, `figures/` should contain, among others:

**Levels and composition**

* `monthly_stack.png` – stacked monthly export values by material group.
* `monthly_share_stack.png` – stacked shares `share_{g,t}` within the tracked basket.
* `annual_stack.png` – annual version of the value stack.

**Concentration**

* `hhi_monthly.png` – monthly HHI across material groups.
* `hhi_components.png` – group contributions to `HHI_t^{group}` via squared shares.
* `effective_groups.png` – effective number of groups, `1/HHI_t^{group}`.
* `hs6_hhi_small_multiples.png` – HS6 HHIs within each group over time.
* `eff_hs6_over_time.png` – effective number of HS6 lines by group and year.

**Growth and regimes**

* `group_growth_scatter.png` – quantity CAGR (2018–2024) vs log final export value by group.
* `price_quantity_regimes.png` – for each group: corr(log quantity, log unit value) vs average absolute monthly price change.

**Event windows (export controls)**

* `event_value_Ga_Ge_licensing_2023-08.png` – gallium/germanium/nickel value indexes around July–August 2023 licensing.
* `event_share_Ga_Ge_licensing_2023-08.png` – share indexes for the same window.
* `event_value_Graphite_licensing_2023-12.png` – graphite value indexes around December 2023 licensing.
* `event_share_Graphite_licensing_2023-12.png` – graphite share indexes.

**Unit value and quantity panels**

For each group `g` (aluminum, copper, graphite, lithium, manganese, nickel, silicon, cobalt, rare_earths, gallium, germanium, anode):

* `unit_value_<group>.png` – monthly unit value `value_{g,t} / qty_{g,t}` and total value.
* `quantity_<group>.png` – monthly quantity series with context.

These underpin the classification into “commodity-like” vs “boom/scarcity” regimes.

---

## What the pipeline is actually computing

At a high level:

* **Group-level trade series**

  * `value_{g,t}`, `qty_{g,t}` from HS6 sums.
  * `share_{g,t} = value_{g,t} / Σ_g value_{g,t}`.
  * Annual aggregates and CAGRs across 2018–2024.

* **Concentration**

  * Across groups: `HHI_t^{group} = Σ_g share_{g,t}^2`, with `N_t^{eff} = 1 / HHI_t^{group}`.
  * Within groups: HS6 shares `s_{g,h,t}` and `HHI_{g,t}^{HS6}`, with `N_{g,t}^{eff-HS6} = 1 / HHI_{g,t}^{HS6}`.

* **Unit values and regimes**

  * `p_{g,t} = value_{g,t} / qty_{g,t}`.
  * For each group:

    * corr(log `qty_{g,t}`, log `p_{g,t}`),
    * average |Δ price| per month.

* **Event windows for export controls**

  * For each policy episode (gallium/germanium licensing, graphite licensing):

    * choose event month `t0`,
    * construct indexes `100 × y_{g,t} / pre-event-mean(y_{g})` in a ±12 month window,
    * plot both value and share indexes.

The resulting figures are intended to answer questions like:

* How fast are specific critical-mineral exports growing relative to traditional base metals?
* How thin is the HS6 product-line structure within each material group?
* Which materials behave like competitive commodities vs speculative, scarcity-driven segments?
* How do the 2023 export-control episodes show up in levels and composition of China’s world-aggregate exports?

## Reproducibility checklist

To fully reproduce the current figures from scratch:

1. Clone the repo and create the environment.
2. Run `pull_from_map.py` to fetch 2018–2024 monthly **export** data for China (`reporter=156`, `partner=0`).
3. Run `etl.py` to generate `materials_*.parquet` and `hs6_hhi.parquet`.
4. Run `make_figures.py` to populate `figures/`.

---



If you use this code or its derived figures in a paper, please cite the corresponding project / paper and this repository.
